{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io, sys\n",
    "import numpy as np\n",
    "from heapq import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vectors(filename):\n",
    "    fin = io.open(filename, 'r', encoding='utf-8', newline='\\n')\n",
    "    n, d = map(int, fin.readline().split())\n",
    "    data = {}\n",
    "    for line in fin:\n",
    "        tokens = line.rstrip().split(' ')\n",
    "        data[tokens[0]] = np.asarray(list(map(float, tokens[1:])))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ** Word vectors ** \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-2.8969e-01, -7.0085e-02,  1.7549e-02, -7.2716e-02,  2.7715e-01,\n",
       "        1.9946e-02,  1.8245e-01, -4.4813e-02,  1.3308e-03,  1.6205e-02,\n",
       "        7.4452e-02,  1.4467e-01,  8.3710e-02, -4.7101e-02, -1.7500e-01,\n",
       "       -3.8751e-01, -1.4386e-01,  8.7873e-02, -1.3676e-01,  1.3489e-01,\n",
       "        2.9965e-01, -3.6715e-02, -1.3844e-01, -3.3033e-01,  3.3351e-03,\n",
       "       -2.6587e-01, -3.3781e-01, -1.0466e-01,  2.1743e-01,  4.9721e-02,\n",
       "        2.8385e-01,  5.7634e-01, -1.7183e-01, -7.9356e-02, -1.3265e-01,\n",
       "       -2.6802e-01,  2.4459e-01, -1.0331e-01,  1.4802e-01, -1.9962e-01,\n",
       "        4.0409e-01, -1.8832e-01, -3.2674e-01,  1.6510e-02,  4.2237e-02,\n",
       "        3.1508e-01,  2.1756e-01,  4.2753e-02,  1.8895e-01,  3.4312e-01,\n",
       "        3.7955e-01, -4.2624e-01, -1.0762e-01, -3.6474e-02, -1.3423e-01,\n",
       "       -4.5829e-01,  1.5198e-01,  2.5223e-01, -2.2773e-01,  3.6631e-01,\n",
       "        2.6299e-01, -4.1108e-03,  1.9334e-01, -2.7285e-02, -1.4374e-01,\n",
       "        1.7114e-02, -2.9621e-01, -1.7492e-01, -8.2981e-02, -1.7569e-01,\n",
       "        6.5224e-03, -1.1263e-01,  3.5393e-01, -1.5335e-01,  4.4194e-01,\n",
       "       -2.8393e-02,  2.5271e-01, -1.1075e-01, -1.3862e-01,  1.1739e-01,\n",
       "        1.5425e-01,  4.2064e-02, -1.2651e-01, -2.8624e-01, -4.6669e-02,\n",
       "       -1.4632e-01, -3.7494e-01,  1.3364e-02,  3.2093e-03, -1.0707e-01,\n",
       "        1.5136e-01, -1.2208e-02,  8.5195e-02,  3.1432e-01,  1.8818e-02,\n",
       "        2.5865e-01,  1.5390e-01, -5.2709e-02, -5.0773e-02, -2.3106e-01,\n",
       "        1.8494e-01, -4.2971e-02, -1.8492e-01, -2.5577e-01,  8.2451e-02,\n",
       "        2.4188e-01, -3.2569e-01, -3.2244e-02,  5.4738e-01, -2.8995e-01,\n",
       "        1.0435e-01,  4.0747e-01, -6.9405e-03, -1.5250e-01,  7.3961e-03,\n",
       "       -7.1185e-02,  1.8417e-01,  2.8079e-01, -1.1042e-01, -3.9004e-02,\n",
       "        7.8953e-02,  1.7002e-02, -1.2561e-01,  2.2142e-01,  6.3944e-02,\n",
       "       -6.4305e-02,  9.9135e-02, -1.6324e-01,  5.9551e-02, -7.3092e-02,\n",
       "        1.2268e-01,  1.5767e-01,  1.8161e-03,  4.1737e-01,  2.0982e-01,\n",
       "        2.0308e-01, -1.5713e-01, -1.1411e-01,  1.4635e-01,  2.5528e-01,\n",
       "       -9.4350e-02, -1.7985e-01,  1.2843e-01,  8.3765e-02,  1.3258e-01,\n",
       "        4.4631e-01,  1.5564e-01, -7.7333e-02,  1.6771e-02, -3.9353e-02,\n",
       "        1.9557e-01, -4.6569e-01, -4.1915e-02, -1.7419e-01,  1.2932e-01,\n",
       "       -9.2549e-02,  1.0880e-01,  2.9053e-02,  1.6280e-01,  4.1763e-01,\n",
       "        6.9828e-02,  4.3012e-02, -6.8687e-02,  1.6339e-01,  9.3486e-02,\n",
       "       -1.6461e-01,  4.5511e-01, -1.4689e-01,  3.2998e-01, -4.9629e-02,\n",
       "       -4.1841e-01,  5.9855e-02, -4.4654e-01, -4.9957e-01,  3.3764e-01,\n",
       "        2.7787e-01,  2.4711e-01, -1.5835e-01,  8.9127e-03, -6.9552e-03,\n",
       "       -2.8542e-01, -1.1491e-01,  1.1683e-01,  4.0556e-01, -5.2455e-02,\n",
       "        5.9876e-01, -7.6273e-02, -6.4377e-02,  1.4786e-01, -1.2219e-01,\n",
       "       -4.6843e-01, -2.2975e-01,  4.2286e-01, -1.0207e-01, -2.9993e-01,\n",
       "       -4.3911e-01,  1.2264e-01,  3.6856e-01, -3.0009e-01, -1.5730e-01,\n",
       "        1.4910e-01, -1.5819e-01, -5.1938e-02, -3.2599e-01,  2.1935e-01,\n",
       "       -1.6303e-01, -7.2895e-02, -8.7542e-02,  3.8412e-01, -2.4773e-01,\n",
       "       -6.7151e-02,  3.1099e-02, -2.2093e-01,  3.2075e-01, -3.1467e-01,\n",
       "       -3.3283e-01, -2.6234e-01,  3.4514e-01, -6.4284e-01,  4.9153e-02,\n",
       "       -3.9607e-01,  6.2140e-02, -4.4424e-02,  1.6845e-01, -2.1241e-01,\n",
       "        4.0302e-01, -2.5669e-01,  1.6817e-03, -6.9202e-02,  1.9496e-01,\n",
       "        1.5301e-01,  7.3950e-02, -1.3820e-01, -3.2782e-01,  2.5533e-01,\n",
       "        2.6074e-01, -8.7477e-02,  1.1091e-01, -1.0670e-01, -5.6014e-02,\n",
       "       -1.9184e-01, -1.0075e-01, -3.3241e-01, -2.6068e-01,  3.3056e-02,\n",
       "       -2.0297e-01, -4.2145e-01, -8.8337e-02, -1.0719e-03,  6.2194e-04,\n",
       "       -1.6053e-01,  6.9839e-02, -5.9904e-02,  2.6794e-01,  3.5577e-01,\n",
       "       -2.7365e-01,  8.4920e-02, -3.3576e-02,  4.7268e-02,  2.4025e-01,\n",
       "       -1.2587e-01,  1.3153e-01,  1.9024e-01,  2.5020e-01,  9.1207e-02,\n",
       "       -1.5277e-02,  1.9384e-01, -6.3305e-02,  5.0978e-04, -1.1223e-01,\n",
       "       -3.6115e-01,  2.5301e-01, -3.1997e-01, -1.6270e-01, -2.8849e-01,\n",
       "       -4.9804e-01,  9.2375e-02, -9.3882e-03,  1.3072e-02, -1.7471e-01,\n",
       "        1.3865e-01,  3.1199e-01,  3.8019e-01, -9.9161e-02,  1.6808e-01,\n",
       "       -1.4326e-02, -7.2776e-02,  8.1448e-02, -2.4118e-01,  2.6518e-01,\n",
       "        1.2751e-01,  9.1295e-02,  1.7135e-01,  1.4421e-01, -3.9970e-02,\n",
       "       -4.1658e-01, -1.5119e-01, -1.9135e-01, -2.5101e-01,  6.1300e-02])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading word vectors\n",
    "\n",
    "print('')\n",
    "print(' ** Word vectors ** ')\n",
    "print('')\n",
    "\n",
    "word_vectors = load_vectors('wiki.en.vec')\n",
    "word_vectors['b']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This function computes the cosine similarity between vectors u and v\n",
    "\n",
    "def cosine(u, v):\n",
    "    ## FILL CODE\n",
    "    cosi= u@v/(np.linalg.norm(u)*np.linalg.norm(v))\n",
    "    return cosi\n",
    "\n",
    "## This function returns the word corresponding to \n",
    "## nearest neighbor vector of x\n",
    "## The list exclude_words can be used to exclude some\n",
    "## words from the nearest neighbors search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "similarity(apple, apples) = 0.637\n",
      "similarity(apple, banana) = 0.431\n",
      "similarity(apple, tiger) = 0.212\n"
     ]
    }
   ],
   "source": [
    "# compute similarity between words\n",
    "\n",
    "print('similarity(apple, apples) = %.3f' %\n",
    "      cosine(word_vectors['apple'], word_vectors['apples']))\n",
    "print('similarity(apple, banana) = %.3f' %\n",
    "      cosine(word_vectors['apple'], word_vectors['banana']))\n",
    "print('similarity(apple, tiger) = %.3f' %\n",
    "      cosine(word_vectors['apple'], word_vectors['tiger']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Functions for nearest neighbors\n",
    "###without the cosine function defineed above but cosine\n",
    "\n",
    "def nearest_neighbor(x, word_vectors, exclude_words=[]):\n",
    "    best_score = -1.0\n",
    "    best_word = ''\n",
    "\n",
    "    ## FILL CODE\n",
    "    for word in word_vectors:\n",
    "        if word in exclude_words:\n",
    "            pass\n",
    "        else:\n",
    "            score =cosine(x, word_vectors[word])\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_word = word\n",
    "            \n",
    "            \n",
    "\n",
    "    return best_word\n",
    "\n",
    "## This function return the words corresponding to the\n",
    "## K nearest neighbors of vector x.\n",
    "## You can use the functions heappush and heappop.\n",
    "\n",
    "def knn(x, vectors, k):\n",
    "    heap = []\n",
    "    for word in vectors:\n",
    "        if len(heap)>=k:\n",
    "            heappush(heap,(cosine(x,vectors[word]),word))\n",
    "            heappop(heap)\n",
    "        else:\n",
    "            heappush(heap,(cosine(x,vectors[word]),word))\n",
    "            \n",
    "            \n",
    "\n",
    "    ## FILL CODE\n",
    "    \n",
    "\n",
    "    return [heappop(heap) for i in range(len(heap))][::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Functions for nearest neighbors\n",
    "\n",
    "def nearest_neighbor(x, word_vectors, exclude_words=[]):\n",
    "    best_score = -1.0\n",
    "    best_word = ''\n",
    "\n",
    "    ## FILL CODE\n",
    "    for word in word_vectors:\n",
    "        if word in exclude_words:\n",
    "            pass\n",
    "        else:\n",
    "            score =cosine(x, word_vectors[word])\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_word = word\n",
    "            \n",
    "            \n",
    "\n",
    "    return best_word\n",
    "\n",
    "## This function return the words corresponding to the\n",
    "## K nearest neighbors of vector x.\n",
    "## You can use the functions heappush and heappop.\n",
    "\n",
    "def knn(x, vectors, k):\n",
    "    heap = []\n",
    "    for word in vectors:\n",
    "        if len(heap)>=k:\n",
    "            heappush(heap,(cosine(x,vectors[word]),word))\n",
    "            heappop(heap)\n",
    "        else:\n",
    "            heappush(heap,(cosine(x,vectors[word]),word))\n",
    "            \n",
    "            \n",
    "\n",
    "    ## FILL CODE\n",
    "    \n",
    "\n",
    "    return [heappop(heap) for i in range(len(heap))][::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The nearest neighbor of cat is: dog\n",
      "\n",
      "cat\n",
      "--------------\n",
      "cat\t1.000\n",
      "cats\t0.732\n",
      "dog\t0.638\n",
      "pet\t0.573\n",
      "rabbit\t0.549\n",
      "dogs\t0.538\n",
      "pig\t0.458\n"
     ]
    }
   ],
   "source": [
    "# looking at nearest neighbors of a word\n",
    "\n",
    "print('The nearest neighbor of cat is: ' +\n",
    "      nearest_neighbor(word_vectors['cat'], word_vectors, ['cat','cats']))\n",
    "\n",
    "knn_cat = knn(word_vectors['cat'], word_vectors, 7)#you can change the number from 7 to increase k\n",
    "print('')\n",
    "print('cat')\n",
    "print('--------------')\n",
    "for score, word in knn(word_vectors['cat'], word_vectors, 7):\n",
    "    print(word + '\\t%.3f' % score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The nearest neighbor of man is: woman\n",
      "\n",
      "woman\n",
      "--------------\n",
      "man\t1.000\n",
      "woman\t0.651\n",
      "stranger\t0.551\n",
      "boy\t0.546\n",
      "spider\t0.537\n",
      "girl\t0.534\n",
      "gentleman\t0.523\n"
     ]
    }
   ],
   "source": [
    "# looking at nearest neighbors of a word\n",
    "\n",
    "print('The nearest neighbor of man is: ' +\n",
    "      nearest_neighbor(word_vectors['woman'], word_vectors, ['cat','cats']))\n",
    "\n",
    "knn_cat = knn(word_vectors['man'], word_vectors, 7)#you can change the number from 7 to increase k\n",
    "print('')\n",
    "print('woman')\n",
    "print('--------------')\n",
    "for score, word in knn(word_vectors['man'], word_vectors, 7):\n",
    "    print(word + '\\t%.3f' % score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This function return the word d, such that a:b and c:d\n",
    "## verifies the same relation\n",
    "\n",
    "def analogy(a, b, c, word_vectors):\n",
    "    a = a.lower()\n",
    "    b = b.lower()\n",
    "    c = c.lower()\n",
    "    xa, xb, xc = word_vectors[a], word_vectors[b], word_vectors[c]\n",
    "    xa = xa/np.linalg.norm(xa)\n",
    "    xb = xb/np.linalg.norm(xb)\n",
    "    xc = xc/np.linalg.norm(xc)\n",
    "    best_score = float('-inf')\n",
    "    best_word = ''\n",
    "    for word in word_vectors:\n",
    "        if word in [a,b,c]:\n",
    "            continue\n",
    "            \n",
    "        vectornorm = word_vectors[word]/np.linalg.norm(word_vectors[word])\n",
    "        distance = (xc - xa + xb).dot(vectornorm)\n",
    "        if distance > best_score:\n",
    "            best_score = distance\n",
    "            best_word = word\n",
    "    ## FILL CODE\n",
    "    \n",
    "    return best_word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "france - paris + rome = italy\n"
     ]
    }
   ],
   "source": [
    "# Word analogies\n",
    "\n",
    "print('')\n",
    "print('france - paris + rome = ' + analogy('paris', 'france', 'rome', word_vectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "king - man + woman = queen\n"
     ]
    }
   ],
   "source": [
    "# Word analogies\n",
    "\n",
    "print('')\n",
    "print('king - man + woman = ' + analogy('man', 'king', 'woman', word_vectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "similarity(genius, man) = 0.445\n",
      "similarity(genius, woman) = 0.325\n"
     ]
    }
   ],
   "source": [
    "## A word about biases in word vectors:\n",
    "\n",
    "print('')\n",
    "print('similarity(genius, man) = %.3f' %\n",
    "      cosine(word_vectors['man'], word_vectors['genius']))\n",
    "print('similarity(genius, woman) = %.3f' %\n",
    "      cosine(word_vectors['woman'], word_vectors['genius']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Compute the association strength between:\n",
    "##   - a word w\n",
    "##   - two sets of attributes A and B\n",
    "\n",
    "def association_strength(w, A, B, vectors):\n",
    "    #strength = 0.0\n",
    "    sum1 = 0.0\n",
    "    sum2 = 0.0\n",
    "    ## FILL CODE\n",
    "    for i in A:\n",
    "        sum1 +=cosine(vectors[w], vectors[i])\n",
    "\n",
    "    for j in B:\n",
    "        sum2 += cosine(vectors[w], vectors[j])\n",
    "    \n",
    "    strength = (1/len(A))*sum1 - (1/len(B)*sum2)\n",
    "    ## FILL CODE\n",
    "    return strength\n",
    "\n",
    "\n",
    "\n",
    "## Perform the word embedding association test between:\n",
    "##   - two sets of words X and Y\n",
    "##   - two sets of attributes A and B\n",
    "\n",
    "\n",
    "def weat(X, Y, A, B, vectors):\n",
    "    score = 0.0\n",
    "    ## FILL CODE\n",
    "    score1 = 0.0\n",
    "    score2 = 0.0\n",
    "    for i in X:\n",
    "        score1 += association_strength(i, A,B, vectors)\n",
    "    \n",
    "    for j in Y:\n",
    "        score2 += association_strength(j, A,B, vectors)\n",
    "    score = score1 - score2\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Word embedding association test: 0.847\n"
     ]
    }
   ],
   "source": [
    "## Replicate one of the experiments from:\n",
    "##\n",
    "## Semantics derived automatically from language corpora contain human-like biases\n",
    "## Caliskan, Bryson, Narayanan (2017)\n",
    "\n",
    "career = ['executive', 'management', 'professional', 'corporation', \n",
    "          'salary', 'office', 'business', 'career']\n",
    "family = ['home', 'parents', 'children', 'family',\n",
    "          'cousins', 'marriage', 'wedding', 'relatives']\n",
    "male = ['john', 'paul', 'mike', 'kevin', 'steve', 'greg', 'jeff', 'bill']\n",
    "female = ['amy', 'joan', 'lisa', 'sarah', 'diana', 'kate', 'ann', 'donna']\n",
    "\n",
    "print('')\n",
    "print('Word embedding association test: %.3f' %\n",
    "      weat(career, family, male, female, word_vectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
