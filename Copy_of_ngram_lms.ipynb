{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    },
    "colab": {
      "name": "Copy of ngram_lms.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/salomeyosei/Natural-Language-Processing/blob/master/Copy_of_ngram_lms.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BdIYalcwxogn",
        "colab_type": "text"
      },
      "source": [
        "# N-Gram Language Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gHQdQmnOxogr",
        "colab_type": "text"
      },
      "source": [
        "#### Preliminaries: Install KenLM\n",
        "\n",
        "[KenLM](https://kheafield.com/code/kenlm/) is used in the latter part of this lab.\n",
        "\n",
        "To install it, run the following commands **in the jupyter terminal** (see screenshot):\n",
        "\n",
        "    sudo apt-get install build-essential libboost-all-dev cmake zlib1g-dev libbz2-dev liblzma-dev\n",
        "    \n",
        "    cd /home/jupyter\n",
        "    wget -O - https://kheafield.com/code/kenlm.tar.gz |tar xz\n",
        "    mkdir kenlm/build\n",
        "    cd kenlm/build\n",
        "    cmake ..\n",
        "    make -j 4\n",
        "    \n",
        "    cd /home/jupyter/kenlm\n",
        "    python setup.py install\n",
        "    \n",
        "<img src=\"img/instruction.png\" alt=\"Drawing\" style=\"width: 35%;\"/>\n",
        "\n",
        "<img src=\"img/instruction2.png\" alt=\"Drawing\" style=\"width: 70%;\"/>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TP2l7iQ7xtPV",
        "colab_type": "code",
        "outputId": "5b77a74e-8797-4682-d44f-a2091e78cfc2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!git clone https://github.com/Kabongosalomon/AMMI-NLP.git\n",
        "\n",
        "%cd AMMI-NLP/Part\\ 02/001_LM/ngram_lm_lab  \n",
        "\n",
        "!sudo apt-get install build-essential libboost-all-dev cmake zlib1g-dev libbz2-dev liblzma-dev\n",
        "\n",
        "!wget -O - https://kheafield.com/code/kenlm.tar.gz |tar xz\n",
        "!mkdir kenlm/build\n",
        "%cd kenlm/build\n",
        "!cmake ..\n",
        "!make -j 4\n",
        "\n",
        "%cd ..\n",
        "\n",
        "! python setup.py install\n",
        "\n",
        "%cd .."
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'AMMI-NLP'...\n",
            "remote: Enumerating objects: 709, done.\u001b[K\n",
            "remote: Total 709 (delta 0), reused 0 (delta 0), pack-reused 709\n",
            "Receiving objects: 100% (709/709), 167.28 MiB | 28.90 MiB/s, done.\n",
            "Resolving deltas: 100% (188/188), done.\n",
            "Checking out files: 100% (464/464), done.\n",
            "/content/AMMI-NLP/Part 02/001_LM/ngram_lm_lab\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "build-essential is already the newest version (12.4ubuntu1).\n",
            "liblzma-dev is already the newest version (5.2.2-1.3).\n",
            "liblzma-dev set to manually installed.\n",
            "zlib1g-dev is already the newest version (1:1.2.11.dfsg-0ubuntu2).\n",
            "zlib1g-dev set to manually installed.\n",
            "libboost-all-dev is already the newest version (1.65.1.0ubuntu1).\n",
            "cmake is already the newest version (3.10.2-1ubuntu2.18.04.1).\n",
            "libbz2-dev is already the newest version (1.0.6-8.1ubuntu0.2).\n",
            "libbz2-dev set to manually installed.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 25 not upgraded.\n",
            "--2020-03-23 09:43:41--  https://kheafield.com/code/kenlm.tar.gz\n",
            "Resolving kheafield.com (kheafield.com)... 35.196.63.85\n",
            "Connecting to kheafield.com (kheafield.com)|35.196.63.85|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 490005 (479K) [application/x-gzip]\n",
            "Saving to: ‘STDOUT’\n",
            "\n",
            "-                   100%[===================>] 478.52K  --.-KB/s    in 0.03s   \n",
            "\n",
            "2020-03-23 09:43:41 (15.5 MB/s) - written to stdout [490005/490005]\n",
            "\n",
            "/content/AMMI-NLP/Part 02/001_LM/ngram_lm_lab/kenlm/build\n",
            "-- The C compiler identification is GNU 7.5.0\n",
            "-- The CXX compiler identification is GNU 7.5.0\n",
            "-- Check for working C compiler: /usr/bin/cc\n",
            "-- Check for working C compiler: /usr/bin/cc -- works\n",
            "-- Detecting C compiler ABI info\n",
            "-- Detecting C compiler ABI info - done\n",
            "-- Detecting C compile features\n",
            "-- Detecting C compile features - done\n",
            "-- Check for working CXX compiler: /usr/bin/c++\n",
            "-- Check for working CXX compiler: /usr/bin/c++ -- works\n",
            "-- Detecting CXX compiler ABI info\n",
            "-- Detecting CXX compiler ABI info - done\n",
            "-- Detecting CXX compile features\n",
            "-- Detecting CXX compile features - done\n",
            "-- Looking for pthread.h\n",
            "-- Looking for pthread.h - found\n",
            "-- Looking for pthread_create\n",
            "-- Looking for pthread_create - not found\n",
            "-- Looking for pthread_create in pthreads\n",
            "-- Looking for pthread_create in pthreads - not found\n",
            "-- Looking for pthread_create in pthread\n",
            "-- Looking for pthread_create in pthread - found\n",
            "-- Found Threads: TRUE  \n",
            "-- Boost version: 1.65.1\n",
            "-- Found the following Boost libraries:\n",
            "--   program_options\n",
            "--   system\n",
            "--   thread\n",
            "--   unit_test_framework\n",
            "--   chrono\n",
            "--   date_time\n",
            "--   atomic\n",
            "-- Found ZLIB: /usr/lib/x86_64-linux-gnu/libz.so (found version \"1.2.11\") \n",
            "-- Found BZip2: /usr/lib/x86_64-linux-gnu/libbz2.so (found version \"1.0.6\") \n",
            "-- Looking for BZ2_bzCompressInit\n",
            "-- Looking for BZ2_bzCompressInit - found\n",
            "-- Looking for lzma_auto_decoder in /usr/lib/x86_64-linux-gnu/liblzma.so\n",
            "-- Looking for lzma_auto_decoder in /usr/lib/x86_64-linux-gnu/liblzma.so - found\n",
            "-- Looking for lzma_easy_encoder in /usr/lib/x86_64-linux-gnu/liblzma.so\n",
            "-- Looking for lzma_easy_encoder in /usr/lib/x86_64-linux-gnu/liblzma.so - found\n",
            "-- Looking for lzma_lzma_preset in /usr/lib/x86_64-linux-gnu/liblzma.so\n",
            "-- Looking for lzma_lzma_preset in /usr/lib/x86_64-linux-gnu/liblzma.so - found\n",
            "-- Found LibLZMA: /usr/include (found version \"5.2.2\") \n",
            "-- Could NOT find Eigen3 (missing: EIGEN3_INCLUDE_DIR EIGEN3_VERSION_OK) (Required is at least version \"2.91.0\")\n",
            "CMake Warning at lm/interpolate/CMakeLists.txt:65 (message):\n",
            "  Not building interpolation.  Eigen3 was not found.\n",
            "\n",
            "\n",
            "-- To install Eigen3 in your home directory, copy paste this:\n",
            "export EIGEN3_ROOT=$HOME/eigen-eigen-07105f7124f9\n",
            "(cd $HOME; wget -O - https://bitbucket.org/eigen/eigen/get/3.2.8.tar.bz2 |tar xj)\n",
            "rm CMakeCache.txt\n",
            "\n",
            "-- Configuring done\n",
            "-- Generating done\n",
            "-- Build files have been written to: /content/AMMI-NLP/Part 02/001_LM/ngram_lm_lab/kenlm/build\n",
            "\u001b[35m\u001b[1mScanning dependencies of target kenlm_filter\u001b[0m\n",
            "\u001b[35m\u001b[1mScanning dependencies of target kenlm_util\u001b[0m\n",
            "[  2%] \u001b[32mBuilding CXX object lm/filter/CMakeFiles/kenlm_filter.dir/vocab.cc.o\u001b[0m\n",
            "[  2%] \u001b[32mBuilding CXX object lm/filter/CMakeFiles/kenlm_filter.dir/phrase.cc.o\u001b[0m\n",
            "[  3%] \u001b[32mBuilding CXX object lm/filter/CMakeFiles/kenlm_filter.dir/arpa_io.cc.o\u001b[0m\n",
            "[  5%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/double-conversion/bignum-dtoa.cc.o\u001b[0m\n",
            "[  6%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/double-conversion/bignum.cc.o\u001b[0m\n",
            "[  7%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/double-conversion/cached-powers.cc.o\u001b[0m\n",
            "[  8%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/double-conversion/diy-fp.cc.o\u001b[0m\n",
            "[ 10%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/double-conversion/double-conversion.cc.o\u001b[0m\n",
            "[ 11%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/double-conversion/fast-dtoa.cc.o\u001b[0m\n",
            "[ 12%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/double-conversion/fixed-dtoa.cc.o\u001b[0m\n",
            "[ 13%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/double-conversion/strtod.cc.o\u001b[0m\n",
            "[ 15%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/stream/chain.cc.o\u001b[0m\n",
            "[ 16%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/stream/count_records.cc.o\u001b[0m\n",
            "[ 17%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/stream/io.cc.o\u001b[0m\n",
            "[ 18%] \u001b[32m\u001b[1mLinking CXX static library ../../lib/libkenlm_filter.a\u001b[0m\n",
            "[ 18%] Built target kenlm_filter\n",
            "[ 20%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/stream/line_input.cc.o\u001b[0m\n",
            "[ 21%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/stream/multi_progress.cc.o\u001b[0m\n",
            "[ 22%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/stream/rewindable_stream.cc.o\u001b[0m\n",
            "[ 23%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/bit_packing.cc.o\u001b[0m\n",
            "[ 25%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/ersatz_progress.cc.o\u001b[0m\n",
            "[ 26%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/exception.cc.o\u001b[0m\n",
            "[ 27%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/file.cc.o\u001b[0m\n",
            "[ 28%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/file_piece.cc.o\u001b[0m\n",
            "[ 30%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/float_to_string.cc.o\u001b[0m\n",
            "[ 31%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/integer_to_string.cc.o\u001b[0m\n",
            "[ 32%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/mmap.cc.o\u001b[0m\n",
            "[ 33%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/murmur_hash.cc.o\u001b[0m\n",
            "[ 35%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/parallel_read.cc.o\u001b[0m\n",
            "[ 36%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/pool.cc.o\u001b[0m\n",
            "[ 37%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/read_compressed.cc.o\u001b[0m\n",
            "[ 38%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/scoped.cc.o\u001b[0m\n",
            "[ 40%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/spaces.cc.o\u001b[0m\n",
            "[ 41%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/string_piece.cc.o\u001b[0m\n",
            "[ 42%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/usage.cc.o\u001b[0m\n",
            "[ 43%] \u001b[32m\u001b[1mLinking CXX static library ../lib/libkenlm_util.a\u001b[0m\n",
            "[ 43%] Built target kenlm_util\n",
            "\u001b[35m\u001b[1mScanning dependencies of target probing_hash_table_benchmark\u001b[0m\n",
            "\u001b[35m\u001b[1mScanning dependencies of target kenlm\u001b[0m\n",
            "[ 45%] \u001b[32mBuilding CXX object util/CMakeFiles/probing_hash_table_benchmark.dir/probing_hash_table_benchmark_main.cc.o\u001b[0m\n",
            "[ 46%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/binary_format.cc.o\u001b[0m\n",
            "[ 47%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/config.cc.o\u001b[0m\n",
            "[ 48%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/bhiksha.cc.o\u001b[0m\n",
            "[ 50%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/lm_exception.cc.o\u001b[0m\n",
            "[ 51%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/model.cc.o\u001b[0m\n",
            "[ 52%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/quantize.cc.o\u001b[0m\n",
            "[ 53%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/read_arpa.cc.o\u001b[0m\n",
            "[ 55%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/search_hashed.cc.o\u001b[0m\n",
            "[ 56%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/search_trie.cc.o\u001b[0m\n",
            "[ 57%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/sizes.cc.o\u001b[0m\n",
            "[ 58%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/trie.cc.o\u001b[0m\n",
            "[ 60%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/trie_sort.cc.o\u001b[0m\n",
            "[ 61%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/value_build.cc.o\u001b[0m\n",
            "[ 62%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/virtual_interface.cc.o\u001b[0m\n",
            "[ 63%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/vocab.cc.o\u001b[0m\n",
            "[ 65%] \u001b[32m\u001b[1mLinking CXX executable ../bin/probing_hash_table_benchmark\u001b[0m\n",
            "[ 66%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/common/model_buffer.cc.o\u001b[0m\n",
            "[ 66%] Built target probing_hash_table_benchmark\n",
            "[ 67%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/common/print.cc.o\u001b[0m\n",
            "[ 68%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/common/renumber.cc.o\u001b[0m\n",
            "[ 70%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/common/size_option.cc.o\u001b[0m\n",
            "[ 71%] \u001b[32m\u001b[1mLinking CXX static library ../lib/libkenlm.a\u001b[0m\n",
            "[ 71%] Built target kenlm\n",
            "\u001b[35m\u001b[1mScanning dependencies of target fragment\u001b[0m\n",
            "\u001b[35m\u001b[1mScanning dependencies of target kenlm_benchmark\u001b[0m\n",
            "\u001b[35m\u001b[1mScanning dependencies of target build_binary\u001b[0m\n",
            "\u001b[35m\u001b[1mScanning dependencies of target query\u001b[0m\n",
            "[ 72%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm_benchmark.dir/kenlm_benchmark_main.cc.o\u001b[0m\n",
            "[ 73%] \u001b[32mBuilding CXX object lm/CMakeFiles/fragment.dir/fragment_main.cc.o\u001b[0m\n",
            "[ 75%] \u001b[32mBuilding CXX object lm/CMakeFiles/build_binary.dir/build_binary_main.cc.o\u001b[0m\n",
            "[ 76%] \u001b[32mBuilding CXX object lm/CMakeFiles/query.dir/query_main.cc.o\u001b[0m\n",
            "[ 77%] \u001b[32m\u001b[1mLinking CXX executable ../bin/fragment\u001b[0m\n",
            "[ 78%] \u001b[32m\u001b[1mLinking CXX executable ../bin/build_binary\u001b[0m\n",
            "[ 78%] Built target fragment\n",
            "\u001b[35m\u001b[1mScanning dependencies of target kenlm_builder\u001b[0m\n",
            "[ 80%] \u001b[32mBuilding CXX object lm/builder/CMakeFiles/kenlm_builder.dir/adjust_counts.cc.o\u001b[0m\n",
            "[ 80%] Built target build_binary\n",
            "[ 81%] \u001b[32mBuilding CXX object lm/builder/CMakeFiles/kenlm_builder.dir/corpus_count.cc.o\u001b[0m\n",
            "[ 82%] \u001b[32m\u001b[1mLinking CXX executable ../bin/query\u001b[0m\n",
            "[ 82%] Built target query\n",
            "[ 83%] \u001b[32mBuilding CXX object lm/builder/CMakeFiles/kenlm_builder.dir/initial_probabilities.cc.o\u001b[0m\n",
            "[ 85%] \u001b[32mBuilding CXX object lm/builder/CMakeFiles/kenlm_builder.dir/interpolate.cc.o\u001b[0m\n",
            "[ 86%] \u001b[32mBuilding CXX object lm/builder/CMakeFiles/kenlm_builder.dir/output.cc.o\u001b[0m\n",
            "[ 87%] \u001b[32mBuilding CXX object lm/builder/CMakeFiles/kenlm_builder.dir/pipeline.cc.o\u001b[0m\n",
            "\u001b[35m\u001b[1mScanning dependencies of target phrase_table_vocab\u001b[0m\n",
            "[ 88%] \u001b[32mBuilding CXX object lm/filter/CMakeFiles/phrase_table_vocab.dir/phrase_table_vocab_main.cc.o\u001b[0m\n",
            "\u001b[35m\u001b[1mScanning dependencies of target filter\u001b[0m\n",
            "[ 90%] \u001b[32mBuilding CXX object lm/filter/CMakeFiles/filter.dir/filter_main.cc.o\u001b[0m\n",
            "[ 91%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/phrase_table_vocab\u001b[0m\n",
            "[ 91%] Built target phrase_table_vocab\n",
            "[ 92%] \u001b[32m\u001b[1mLinking CXX static library ../../lib/libkenlm_builder.a\u001b[0m\n",
            "[ 92%] Built target kenlm_builder\n",
            "\u001b[35m\u001b[1mScanning dependencies of target count_ngrams\u001b[0m\n",
            "\u001b[35m\u001b[1mScanning dependencies of target lmplz\u001b[0m\n",
            "[ 93%] \u001b[32mBuilding CXX object lm/builder/CMakeFiles/lmplz.dir/lmplz_main.cc.o\u001b[0m\n",
            "[ 95%] \u001b[32mBuilding CXX object lm/builder/CMakeFiles/count_ngrams.dir/count_ngrams_main.cc.o\u001b[0m\n",
            "[ 96%] \u001b[32m\u001b[1mLinking CXX executable ../bin/kenlm_benchmark\u001b[0m\n",
            "[ 96%] Built target kenlm_benchmark\n",
            "[ 97%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/lmplz\u001b[0m\n",
            "[ 97%] Built target lmplz\n",
            "[ 98%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/filter\u001b[0m\n",
            "[ 98%] Built target filter\n",
            "[100%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/count_ngrams\u001b[0m\n",
            "[100%] Built target count_ngrams\n",
            "/content/AMMI-NLP/Part 02/001_LM/ngram_lm_lab/kenlm\n",
            "running install\n",
            "running bdist_egg\n",
            "running egg_info\n",
            "creating kenlm.egg-info\n",
            "writing kenlm.egg-info/PKG-INFO\n",
            "writing dependency_links to kenlm.egg-info/dependency_links.txt\n",
            "writing top-level names to kenlm.egg-info/top_level.txt\n",
            "writing manifest file 'kenlm.egg-info/SOURCES.txt'\n",
            "reading manifest template 'MANIFEST.in'\n",
            "writing manifest file 'kenlm.egg-info/SOURCES.txt'\n",
            "installing library code to build/bdist.linux-x86_64/egg\n",
            "running install_lib\n",
            "running build_ext\n",
            "building 'kenlm' extension\n",
            "creating build/temp.linux-x86_64-3.6\n",
            "creating build/temp.linux-x86_64-3.6/util\n",
            "creating build/temp.linux-x86_64-3.6/lm\n",
            "creating build/temp.linux-x86_64-3.6/util/double-conversion\n",
            "creating build/temp.linux-x86_64-3.6/python\n",
            "x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I. -I/usr/include/python3.6m -c util/scoped.cc -o build/temp.linux-x86_64-3.6/util/scoped.o -O3 -DNDEBUG -DKENLM_MAX_ORDER=6 -std=c++11 -DHAVE_ZLIB -DHAVE_BZLIB -DHAVE_XZLIB\n",
            "x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I. -I/usr/include/python3.6m -c util/parallel_read.cc -o build/temp.linux-x86_64-3.6/util/parallel_read.o -O3 -DNDEBUG -DKENLM_MAX_ORDER=6 -std=c++11 -DHAVE_ZLIB -DHAVE_BZLIB -DHAVE_XZLIB\n",
            "x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I. -I/usr/include/python3.6m -c util/pool.cc -o build/temp.linux-x86_64-3.6/util/pool.o -O3 -DNDEBUG -DKENLM_MAX_ORDER=6 -std=c++11 -DHAVE_ZLIB -DHAVE_BZLIB -DHAVE_XZLIB\n",
            "x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I. -I/usr/include/python3.6m -c util/ersatz_progress.cc -o build/temp.linux-x86_64-3.6/util/ersatz_progress.o -O3 -DNDEBUG -DKENLM_MAX_ORDER=6 -std=c++11 -DHAVE_ZLIB -DHAVE_BZLIB -DHAVE_XZLIB\n",
            "x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I. -I/usr/include/python3.6m -c util/file.cc -o build/temp.linux-x86_64-3.6/util/file.o -O3 -DNDEBUG -DKENLM_MAX_ORDER=6 -std=c++11 -DHAVE_ZLIB -DHAVE_BZLIB -DHAVE_XZLIB\n",
            "x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I. -I/usr/include/python3.6m -c util/murmur_hash.cc -o build/temp.linux-x86_64-3.6/util/murmur_hash.o -O3 -DNDEBUG -DKENLM_MAX_ORDER=6 -std=c++11 -DHAVE_ZLIB -DHAVE_BZLIB -DHAVE_XZLIB\n",
            "x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I. -I/usr/include/python3.6m -c util/spaces.cc -o build/temp.linux-x86_64-3.6/util/spaces.o -O3 -DNDEBUG -DKENLM_MAX_ORDER=6 -std=c++11 -DHAVE_ZLIB -DHAVE_BZLIB -DHAVE_XZLIB\n",
            "x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I. -I/usr/include/python3.6m -c util/exception.cc -o build/temp.linux-x86_64-3.6/util/exception.o -O3 -DNDEBUG -DKENLM_MAX_ORDER=6 -std=c++11 -DHAVE_ZLIB -DHAVE_BZLIB -DHAVE_XZLIB\n",
            "x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I. -I/usr/include/python3.6m -c util/mmap.cc -o build/temp.linux-x86_64-3.6/util/mmap.o -O3 -DNDEBUG -DKENLM_MAX_ORDER=6 -std=c++11 -DHAVE_ZLIB -DHAVE_BZLIB -DHAVE_XZLIB\n",
            "x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I. -I/usr/include/python3.6m -c util/read_compressed.cc -o build/temp.linux-x86_64-3.6/util/read_compressed.o -O3 -DNDEBUG -DKENLM_MAX_ORDER=6 -std=c++11 -DHAVE_ZLIB -DHAVE_BZLIB -DHAVE_XZLIB\n",
            "x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I. -I/usr/include/python3.6m -c util/string_piece.cc -o build/temp.linux-x86_64-3.6/util/string_piece.o -O3 -DNDEBUG -DKENLM_MAX_ORDER=6 -std=c++11 -DHAVE_ZLIB -DHAVE_BZLIB -DHAVE_XZLIB\n",
            "x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I. -I/usr/include/python3.6m -c util/integer_to_string.cc -o build/temp.linux-x86_64-3.6/util/integer_to_string.o -O3 -DNDEBUG -DKENLM_MAX_ORDER=6 -std=c++11 -DHAVE_ZLIB -DHAVE_BZLIB -DHAVE_XZLIB\n",
            "x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I. -I/usr/include/python3.6m -c util/bit_packing.cc -o build/temp.linux-x86_64-3.6/util/bit_packing.o -O3 -DNDEBUG -DKENLM_MAX_ORDER=6 -std=c++11 -DHAVE_ZLIB -DHAVE_BZLIB -DHAVE_XZLIB\n",
            "x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I. -I/usr/include/python3.6m -c util/usage.cc -o build/temp.linux-x86_64-3.6/util/usage.o -O3 -DNDEBUG -DKENLM_MAX_ORDER=6 -std=c++11 -DHAVE_ZLIB -DHAVE_BZLIB -DHAVE_XZLIB\n",
            "x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I. -I/usr/include/python3.6m -c util/file_piece.cc -o build/temp.linux-x86_64-3.6/util/file_piece.o -O3 -DNDEBUG -DKENLM_MAX_ORDER=6 -std=c++11 -DHAVE_ZLIB -DHAVE_BZLIB -DHAVE_XZLIB\n",
            "x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I. -I/usr/include/python3.6m -c util/float_to_string.cc -o build/temp.linux-x86_64-3.6/util/float_to_string.o -O3 -DNDEBUG -DKENLM_MAX_ORDER=6 -std=c++11 -DHAVE_ZLIB -DHAVE_BZLIB -DHAVE_XZLIB\n",
            "x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I. -I/usr/include/python3.6m -c lm/value_build.cc -o build/temp.linux-x86_64-3.6/lm/value_build.o -O3 -DNDEBUG -DKENLM_MAX_ORDER=6 -std=c++11 -DHAVE_ZLIB -DHAVE_BZLIB -DHAVE_XZLIB\n",
            "x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I. -I/usr/include/python3.6m -c lm/search_trie.cc -o build/temp.linux-x86_64-3.6/lm/search_trie.o -O3 -DNDEBUG -DKENLM_MAX_ORDER=6 -std=c++11 -DHAVE_ZLIB -DHAVE_BZLIB -DHAVE_XZLIB\n",
            "x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I. -I/usr/include/python3.6m -c lm/vocab.cc -o build/temp.linux-x86_64-3.6/lm/vocab.o -O3 -DNDEBUG -DKENLM_MAX_ORDER=6 -std=c++11 -DHAVE_ZLIB -DHAVE_BZLIB -DHAVE_XZLIB\n",
            "x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I. -I/usr/include/python3.6m -c lm/model.cc -o build/temp.linux-x86_64-3.6/lm/model.o -O3 -DNDEBUG -DKENLM_MAX_ORDER=6 -std=c++11 -DHAVE_ZLIB -DHAVE_BZLIB -DHAVE_XZLIB\n",
            "x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I. -I/usr/include/python3.6m -c lm/quantize.cc -o build/temp.linux-x86_64-3.6/lm/quantize.o -O3 -DNDEBUG -DKENLM_MAX_ORDER=6 -std=c++11 -DHAVE_ZLIB -DHAVE_BZLIB -DHAVE_XZLIB\n",
            "x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I. -I/usr/include/python3.6m -c lm/lm_exception.cc -o build/temp.linux-x86_64-3.6/lm/lm_exception.o -O3 -DNDEBUG -DKENLM_MAX_ORDER=6 -std=c++11 -DHAVE_ZLIB -DHAVE_BZLIB -DHAVE_XZLIB\n",
            "x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I. -I/usr/include/python3.6m -c lm/trie_sort.cc -o build/temp.linux-x86_64-3.6/lm/trie_sort.o -O3 -DNDEBUG -DKENLM_MAX_ORDER=6 -std=c++11 -DHAVE_ZLIB -DHAVE_BZLIB -DHAVE_XZLIB\n",
            "x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I. -I/usr/include/python3.6m -c lm/binary_format.cc -o build/temp.linux-x86_64-3.6/lm/binary_format.o -O3 -DNDEBUG -DKENLM_MAX_ORDER=6 -std=c++11 -DHAVE_ZLIB -DHAVE_BZLIB -DHAVE_XZLIB\n",
            "x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I. -I/usr/include/python3.6m -c lm/sizes.cc -o build/temp.linux-x86_64-3.6/lm/sizes.o -O3 -DNDEBUG -DKENLM_MAX_ORDER=6 -std=c++11 -DHAVE_ZLIB -DHAVE_BZLIB -DHAVE_XZLIB\n",
            "x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I. -I/usr/include/python3.6m -c lm/bhiksha.cc -o build/temp.linux-x86_64-3.6/lm/bhiksha.o -O3 -DNDEBUG -DKENLM_MAX_ORDER=6 -std=c++11 -DHAVE_ZLIB -DHAVE_BZLIB -DHAVE_XZLIB\n",
            "x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I. -I/usr/include/python3.6m -c lm/config.cc -o build/temp.linux-x86_64-3.6/lm/config.o -O3 -DNDEBUG -DKENLM_MAX_ORDER=6 -std=c++11 -DHAVE_ZLIB -DHAVE_BZLIB -DHAVE_XZLIB\n",
            "x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I. -I/usr/include/python3.6m -c lm/virtual_interface.cc -o build/temp.linux-x86_64-3.6/lm/virtual_interface.o -O3 -DNDEBUG -DKENLM_MAX_ORDER=6 -std=c++11 -DHAVE_ZLIB -DHAVE_BZLIB -DHAVE_XZLIB\n",
            "x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I. -I/usr/include/python3.6m -c lm/trie.cc -o build/temp.linux-x86_64-3.6/lm/trie.o -O3 -DNDEBUG -DKENLM_MAX_ORDER=6 -std=c++11 -DHAVE_ZLIB -DHAVE_BZLIB -DHAVE_XZLIB\n",
            "x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I. -I/usr/include/python3.6m -c lm/read_arpa.cc -o build/temp.linux-x86_64-3.6/lm/read_arpa.o -O3 -DNDEBUG -DKENLM_MAX_ORDER=6 -std=c++11 -DHAVE_ZLIB -DHAVE_BZLIB -DHAVE_XZLIB\n",
            "x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I. -I/usr/include/python3.6m -c lm/search_hashed.cc -o build/temp.linux-x86_64-3.6/lm/search_hashed.o -O3 -DNDEBUG -DKENLM_MAX_ORDER=6 -std=c++11 -DHAVE_ZLIB -DHAVE_BZLIB -DHAVE_XZLIB\n",
            "x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I. -I/usr/include/python3.6m -c util/double-conversion/double-conversion.cc -o build/temp.linux-x86_64-3.6/util/double-conversion/double-conversion.o -O3 -DNDEBUG -DKENLM_MAX_ORDER=6 -std=c++11 -DHAVE_ZLIB -DHAVE_BZLIB -DHAVE_XZLIB\n",
            "x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I. -I/usr/include/python3.6m -c util/double-conversion/fixed-dtoa.cc -o build/temp.linux-x86_64-3.6/util/double-conversion/fixed-dtoa.o -O3 -DNDEBUG -DKENLM_MAX_ORDER=6 -std=c++11 -DHAVE_ZLIB -DHAVE_BZLIB -DHAVE_XZLIB\n",
            "x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I. -I/usr/include/python3.6m -c util/double-conversion/diy-fp.cc -o build/temp.linux-x86_64-3.6/util/double-conversion/diy-fp.o -O3 -DNDEBUG -DKENLM_MAX_ORDER=6 -std=c++11 -DHAVE_ZLIB -DHAVE_BZLIB -DHAVE_XZLIB\n",
            "x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I. -I/usr/include/python3.6m -c util/double-conversion/strtod.cc -o build/temp.linux-x86_64-3.6/util/double-conversion/strtod.o -O3 -DNDEBUG -DKENLM_MAX_ORDER=6 -std=c++11 -DHAVE_ZLIB -DHAVE_BZLIB -DHAVE_XZLIB\n",
            "x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I. -I/usr/include/python3.6m -c util/double-conversion/cached-powers.cc -o build/temp.linux-x86_64-3.6/util/double-conversion/cached-powers.o -O3 -DNDEBUG -DKENLM_MAX_ORDER=6 -std=c++11 -DHAVE_ZLIB -DHAVE_BZLIB -DHAVE_XZLIB\n",
            "x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I. -I/usr/include/python3.6m -c util/double-conversion/bignum-dtoa.cc -o build/temp.linux-x86_64-3.6/util/double-conversion/bignum-dtoa.o -O3 -DNDEBUG -DKENLM_MAX_ORDER=6 -std=c++11 -DHAVE_ZLIB -DHAVE_BZLIB -DHAVE_XZLIB\n",
            "x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I. -I/usr/include/python3.6m -c util/double-conversion/bignum.cc -o build/temp.linux-x86_64-3.6/util/double-conversion/bignum.o -O3 -DNDEBUG -DKENLM_MAX_ORDER=6 -std=c++11 -DHAVE_ZLIB -DHAVE_BZLIB -DHAVE_XZLIB\n",
            "x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I. -I/usr/include/python3.6m -c util/double-conversion/fast-dtoa.cc -o build/temp.linux-x86_64-3.6/util/double-conversion/fast-dtoa.o -O3 -DNDEBUG -DKENLM_MAX_ORDER=6 -std=c++11 -DHAVE_ZLIB -DHAVE_BZLIB -DHAVE_XZLIB\n",
            "x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I. -I/usr/include/python3.6m -c python/score_sentence.cc -o build/temp.linux-x86_64-3.6/python/score_sentence.o -O3 -DNDEBUG -DKENLM_MAX_ORDER=6 -std=c++11 -DHAVE_ZLIB -DHAVE_BZLIB -DHAVE_XZLIB\n",
            "x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I. -I/usr/include/python3.6m -c python/kenlm.cpp -o build/temp.linux-x86_64-3.6/python/kenlm.o -O3 -DNDEBUG -DKENLM_MAX_ORDER=6 -std=c++11 -DHAVE_ZLIB -DHAVE_BZLIB -DHAVE_XZLIB\n",
            "creating build/lib.linux-x86_64-3.6\n",
            "x86_64-linux-gnu-gcc -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.6/util/scoped.o build/temp.linux-x86_64-3.6/util/parallel_read.o build/temp.linux-x86_64-3.6/util/pool.o build/temp.linux-x86_64-3.6/util/ersatz_progress.o build/temp.linux-x86_64-3.6/util/file.o build/temp.linux-x86_64-3.6/util/murmur_hash.o build/temp.linux-x86_64-3.6/util/spaces.o build/temp.linux-x86_64-3.6/util/exception.o build/temp.linux-x86_64-3.6/util/mmap.o build/temp.linux-x86_64-3.6/util/read_compressed.o build/temp.linux-x86_64-3.6/util/string_piece.o build/temp.linux-x86_64-3.6/util/integer_to_string.o build/temp.linux-x86_64-3.6/util/bit_packing.o build/temp.linux-x86_64-3.6/util/usage.o build/temp.linux-x86_64-3.6/util/file_piece.o build/temp.linux-x86_64-3.6/util/float_to_string.o build/temp.linux-x86_64-3.6/lm/value_build.o build/temp.linux-x86_64-3.6/lm/search_trie.o build/temp.linux-x86_64-3.6/lm/vocab.o build/temp.linux-x86_64-3.6/lm/model.o build/temp.linux-x86_64-3.6/lm/quantize.o build/temp.linux-x86_64-3.6/lm/lm_exception.o build/temp.linux-x86_64-3.6/lm/trie_sort.o build/temp.linux-x86_64-3.6/lm/binary_format.o build/temp.linux-x86_64-3.6/lm/sizes.o build/temp.linux-x86_64-3.6/lm/bhiksha.o build/temp.linux-x86_64-3.6/lm/config.o build/temp.linux-x86_64-3.6/lm/virtual_interface.o build/temp.linux-x86_64-3.6/lm/trie.o build/temp.linux-x86_64-3.6/lm/read_arpa.o build/temp.linux-x86_64-3.6/lm/search_hashed.o build/temp.linux-x86_64-3.6/util/double-conversion/double-conversion.o build/temp.linux-x86_64-3.6/util/double-conversion/fixed-dtoa.o build/temp.linux-x86_64-3.6/util/double-conversion/diy-fp.o build/temp.linux-x86_64-3.6/util/double-conversion/strtod.o build/temp.linux-x86_64-3.6/util/double-conversion/cached-powers.o build/temp.linux-x86_64-3.6/util/double-conversion/bignum-dtoa.o build/temp.linux-x86_64-3.6/util/double-conversion/bignum.o build/temp.linux-x86_64-3.6/util/double-conversion/fast-dtoa.o build/temp.linux-x86_64-3.6/python/score_sentence.o build/temp.linux-x86_64-3.6/python/kenlm.o -lstdc++ -lrt -lz -lbz2 -llzma -o build/lib.linux-x86_64-3.6/kenlm.cpython-36m-x86_64-linux-gnu.so\n",
            "creating build/bdist.linux-x86_64\n",
            "creating build/bdist.linux-x86_64/egg\n",
            "copying build/lib.linux-x86_64-3.6/kenlm.cpython-36m-x86_64-linux-gnu.so -> build/bdist.linux-x86_64/egg\n",
            "creating stub loader for kenlm.cpython-36m-x86_64-linux-gnu.so\n",
            "byte-compiling build/bdist.linux-x86_64/egg/kenlm.py to kenlm.cpython-36.pyc\n",
            "creating build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying kenlm.egg-info/PKG-INFO -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying kenlm.egg-info/SOURCES.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying kenlm.egg-info/dependency_links.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying kenlm.egg-info/top_level.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "writing build/bdist.linux-x86_64/egg/EGG-INFO/native_libs.txt\n",
            "zip_safe flag not set; analyzing archive contents...\n",
            "__pycache__.kenlm.cpython-36: module references __file__\n",
            "creating dist\n",
            "creating 'dist/kenlm-0.0.0-py3.6-linux-x86_64.egg' and adding 'build/bdist.linux-x86_64/egg' to it\n",
            "removing 'build/bdist.linux-x86_64/egg' (and everything under it)\n",
            "Processing kenlm-0.0.0-py3.6-linux-x86_64.egg\n",
            "creating /usr/local/lib/python3.6/dist-packages/kenlm-0.0.0-py3.6-linux-x86_64.egg\n",
            "Extracting kenlm-0.0.0-py3.6-linux-x86_64.egg to /usr/local/lib/python3.6/dist-packages\n",
            "Adding kenlm 0.0.0 to easy-install.pth file\n",
            "\n",
            "Installed /usr/local/lib/python3.6/dist-packages/kenlm-0.0.0-py3.6-linux-x86_64.egg\n",
            "Processing dependencies for kenlm==0.0.0\n",
            "Finished processing dependencies for kenlm==0.0.0\n",
            "/content/AMMI-NLP/Part 02/001_LM/ngram_lm_lab\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6SRqMZRnxogt",
        "colab_type": "code",
        "outputId": "e61d7ddf-350b-477e-caef-5ec5174ac47a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "!python setup.py install"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "python3: can't open file 'setup.py': [Errno 2] No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sXmghFdoxogx",
        "colab_type": "code",
        "outputId": "1bddaee1-1969-4e59-a0a5-b9f985675b55",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        }
      },
      "source": [
        "%cd AMMI-NLP/Part\\ 02/001_LM/ngram_lm_lab  \n",
        "%pylab inline\n",
        "import os, sys, glob, json, math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from pprint import pprint\n",
        "from collections import defaultdict\n",
        "pd.set_option('display.max_colwidth', -1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/AMMI-NLP/Part 02/001_LM/ngram_lm_lab\n",
            "Populating the interactive namespace from numpy and matplotlib\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/IPython/core/magics/pylab.py:161: UserWarning: pylab import has clobbered these variables: ['f']\n",
            "`%matplotlib` prevents importing * from pylab and numpy\n",
            "  \"\\n`%matplotlib` prevents importing * from pylab and numpy\"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NX9kfbr4xog1",
        "colab_type": "text"
      },
      "source": [
        "# Outline:\n",
        "> ### 1. Introduction \n",
        "> ### 2. Language Modeling\n",
        ">> ####  2.1 N-gram language modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NFM5eK_8xog2",
        "colab_type": "text"
      },
      "source": [
        "### 1. Introduction \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dZ6jgLa7xog3",
        "colab_type": "text"
      },
      "source": [
        "## 2. Language Modeling:\n",
        "- Sentence is a sequence of tokens.\n",
        "- Vocabulary is a set of unique tokens.\n",
        "\n",
        " \n",
        "\n",
        "- In **language modeling** we want to model the probability of variable length sequences .i.e. estimate the probability of the sentence by estimate the probability of token given the past,  \n",
        "          \n",
        "         \n",
        "$$ \\text{Input (sentence)} \\rightarrow \\text{Output(probability of the sentence)}$$\n",
        "\n",
        "$$\\large p(W)= p(w_1,\\ldots,w_T).$$\n",
        "\n",
        "- **language modeling** is unsupervised learning so we convert it to sequence of supervised  learning where the label come from the sentence directly,\n",
        "\n",
        "$$\\large p(w_1,\\ldots,w_T)= p(w_1) p(w_2|w_1), \\ldots, p(w_T|w_1,\\ldots,w_{T-1})\\\\ =\\prod_{t=1}^T p(w_t|w_{<t}).$$\n",
        "\n",
        "- input\n",
        "- Table lookup (word embedding which turn discreate token to continous vector)\n",
        "- sentence representation.\n",
        "-  Arbitrary supgraph (to do deep neurel network we need some paprameters $\\theta$), the ouput will be $\\mathbb{R}^{|v|}$.\n",
        "- Softmax over all possible tokens give the conditional distribution $p(w_t|w_{<t}).$\n",
        "- Loss function: the sum of negative log-probabilities\n",
        "    $$\\log p_{\\theta}(w_1,\\ldots,w_T) = \\sum_{n=1}^{N} \\sum_{t=1}^{T} \\log p_{\\theta}(w_t|w_{<t}).$$\n",
        "![Lanuage Modelling](https://raw.githubusercontent.com/AishaAlaagib/NLP/master/language.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9lvlIn8Mxog4",
        "colab_type": "text"
      },
      "source": [
        "### N-gram Language Modeling\n",
        "An **n-gram language model** assumes that each word $w_t$ only depends on the preceding $n-1$ words,\n",
        "- \"n\" refers to the size of past.\n",
        "- **Unigarm**\n",
        "    $$\\large p(w_1,\\ldots,w_T)=\\prod_{t=1}^T p(w_t) = \\prod_{t=1}^T \\frac{c(w_t)}{N}$$ \n",
        "- **Bigram**\n",
        "    $$\\large p(w_1,\\ldots,w_T)=\\prod_{t=1}^T p(w_t|w_{t-1}) = \\prod_{t=1}^T \\frac{c(w_t, w_{t-1})}{c(w_{t-1})}$$\n",
        "- **n-gram** \n",
        "$$\\large p(w_1,\\ldots,w_T)=\\prod_{t=1}^T p(w_t|w_{t-n+1},\\ldots,w_{t-1}).$$\n",
        "\n",
        " \n",
        "\n",
        "#### Example\n",
        "For instance, when modeling the sentence $$\\texttt{the cat sat on the mat .}$$ a 3-gram language model assumes that $$p(\\texttt{mat}|\\texttt{the cat sat on the}) \\approx p(\\texttt{mat}|\\texttt{on the}).$$\n",
        "\n",
        "The sub-sequence $(\\texttt{on the mat})$ is a *3-gram* or *trigram*.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z2iXn5nIxog5",
        "colab_type": "text"
      },
      "source": [
        "### Count-based Estimation\n",
        "\n",
        "Given some dataset $D$ of sequences, we can estimate an n-gram model through counting, derived as follows:\n",
        "\n",
        "\\begin{align}\n",
        "p(w_t|w_{t-n+1},\\ldots,w_{t-1}) &= \\frac{p(w_{t-n+1},\\ldots,w_t)}{p(w_{t-n+1},\\ldots,w_{t-1})} & \\text{definition of conditional probability}\\\\\n",
        "                       &= \\frac{p(w_{t-n+1},\\ldots,w_t)}{\\sum_{w_{t'}}p(w_{t-n+1},\\ldots,w_{t-1},w_{t'})}\\\\\n",
        "                       &\\approx \\frac{\\frac{1}{N}\\text{count}(w_{t-n+1},\\ldots,w_t)}{\\frac{1}{N}\\sum_{w_{t'}}\\text{count}(w_{t-n+1},\\ldots,w_{t-1},w_{t'})}\\\\\n",
        "                       &= \\frac{\\text{count}(w_{t-n+1},\\ldots,w_t)}{\\sum_{w_{t'}}\\text{count}(w_{t-n+1},\\ldots,w_{t-1},w_{t'})}\\\\\n",
        "                       &= \\frac{\\text{count}(w_{t-n+1},\\ldots,w_t)}{\\text{count}(w_{t-n+1},\\ldots,w_{t-1})},\n",
        "\\end{align}\n",
        "\n",
        "where $N$ is the number of $n$-grams in the dataset.\n",
        "\n",
        "In Python, we can collect these counts into a dictionary mapping a prefix to a dictionary of counts:\n",
        "\n",
        "        count[(w_n+1,...,w_t-1)] = {wt1: count of (w_n+1,...,w_t1),\n",
        "                                    wt2: count of (w_n+1,...,w_t2),\n",
        "                                    ...\n",
        "                                   }\n",
        "                                   \n",
        "and for the denominator, maintain a dictionary of totals:\n",
        "\n",
        "        total[(w_n+1,...,w_t-1)] = count of w_n+1,...,w_t-1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K8Y9cx3pxog6",
        "colab_type": "raw"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R0sWvwDHxog7",
        "colab_type": "text"
      },
      "source": [
        "### Simplified Language\n",
        "\n",
        "To get intuition, lets start by modeling a simple language called `ABC`. A word in this language is one of three tokens, $$w\\in \\{\\texttt{A, B, C}\\},$$\n",
        "and we'll denote a sentence as $\\textbf{w}=(w_1,\\ldots,w_{|\\textbf{w}|})$.\n",
        "\n",
        "\n",
        "Suppose we are given the following dataset, and want to estimate a **bigram model**: $$p(\\textbf{w})\\approx\\prod_{t=1}^{|\\textbf{w}|}p(w_t|w_{t-1})\\quad\\quad(*)$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c8ixh5OLxog8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_raw = [['A', 'A', 'B', 'B'],\n",
        "            ['A', 'A', 'B'],\n",
        "            ['A', 'A', 'B', 'C'],\n",
        "            ['A', 'A', 'A'],\n",
        "            ['A', 'A', 'A', 'A']]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qPy8sD68xog_",
        "colab_type": "text"
      },
      "source": [
        "Since our model is a probability distribution, the total probability of all possible strings in the language must sum to 1, i.e.: $$\\sum_{\\textbf{w}}p(\\textbf{w})=1.$$\n",
        "\n",
        "In order to satisfy this criterion it turns out that we need an additional **beginning token**, `<bos>`, and **end token**, `<eos>`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "636GfeYAxohA",
        "colab_type": "code",
        "outputId": "b7966111-d26a-491f-83bb-412d8d71d9a4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        }
      },
      "source": [
        "data = [['<bos>'] + d + ['<eos>'] for d in data_raw]\n",
        "data"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['<bos>', 'A', 'A', 'B', 'B', '<eos>'],\n",
              " ['<bos>', 'A', 'A', 'B', '<eos>'],\n",
              " ['<bos>', 'A', 'A', 'B', 'C', '<eos>'],\n",
              " ['<bos>', 'A', 'A', 'A', '<eos>'],\n",
              " ['<bos>', 'A', 'A', 'A', 'A', '<eos>']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9OMF97pxohD",
        "colab_type": "text"
      },
      "source": [
        "Now let's estimate a bigram model:\n",
        "\n",
        "\\begin{align}\n",
        "p(w_t|w_{t-1}) &= \\frac{\\text{count}(w_{t-1}w_{t})}{\\sum_{w_{t'}}\\text{count}(w_{t-1}w_{t'})}\\\\\n",
        "               &= \\texttt{count[prefix][wt] / totals[prefix]}\n",
        "\\end{align} \n",
        "\n",
        "where $\\texttt{prefix}$ is $w_{t-1}$ in this case."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bw20BsuAxohE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "count = defaultdict(lambda: defaultdict(float))\n",
        "total = defaultdict(float)\n",
        "\n",
        "n = 2\n",
        "for sequence in data:\n",
        "    for i in range(len(sequence)-n+1):         # for each ngram\n",
        "        ngram = tuple(sequence[i:i+n])\n",
        "        prefix, word = ngram[:-1], ngram[-1]\n",
        "        count[prefix][word] += 1               # count(w_{t-n+1}...w_t)\n",
        "        total[prefix] += 1                     # count(w_{t-n+1}...w_{t-1})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ePUgtj3HxohG",
        "colab_type": "text"
      },
      "source": [
        "Let's see if the counts and totals make sense:\n",
        "\n",
        "- How many times did (A, B) occur? What about (B, B)?\n",
        "- How many times did (A) occur? What about (C)?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6vkLWhYLxohH",
        "colab_type": "code",
        "outputId": "8bcc67f3-9f26-4022-f721-6bf2a104e5d2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 161
        }
      },
      "source": [
        "print(\"Counts:\")\n",
        "pprint(dict(count))\n",
        "print(\"\\nTotals:\")\n",
        "pprint(dict(total))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Counts:\n",
            "{('<bos>',): defaultdict(<class 'float'>, {'A': 5.0}),\n",
            " ('A',): defaultdict(<class 'float'>, {'A': 8.0, 'B': 3.0, '<eos>': 2.0}),\n",
            " ('B',): defaultdict(<class 'float'>, {'B': 1.0, '<eos>': 2.0, 'C': 1.0}),\n",
            " ('C',): defaultdict(<class 'float'>, {'<eos>': 1.0})}\n",
            "\n",
            "Totals:\n",
            "{('<bos>',): 5.0, ('A',): 13.0, ('B',): 4.0, ('C',): 1.0}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oP5fhWWnxohL",
        "colab_type": "text"
      },
      "source": [
        "#### Conditional probability queries\n",
        "\n",
        "We can now query a conditional probability:\n",
        "\n",
        "\\begin{align}\n",
        "\\texttt{p(word|prefix)} =&\\ \\texttt{count[prefix][word] / totals[prefix]}\n",
        "\\end{align}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5bRCjnt8xohM",
        "colab_type": "code",
        "outputId": "7cd4b5bf-2b9a-497c-a62c-0eae89caba5c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "queries = [('<bos>', 'A'),\n",
        "           ('B', 'C')]\n",
        "\n",
        "for query in queries:\n",
        "    prefix, word = query[:-1], query[-1]\n",
        "    p = count[prefix][word] / total[prefix]  # We'll discuss the case when `total[prefix] = 0` below.\n",
        "    print(\"p( %s | %s) = \\t%.5f\" % (word, ', '.join(prefix), p))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "p( A | <bos>) = \t1.00000\n",
            "p( C | B) = \t0.25000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ur6YPbpgxohR",
        "colab_type": "text"
      },
      "source": [
        "**Exercise**: Look at the training set and convince yourself that these conditional probabilities are correct according to the count-based estimation procedure."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZS_k9zXZxohS",
        "colab_type": "text"
      },
      "source": [
        "#### Sequence Probability\n",
        "\n",
        "We can compute the probability of a sequence using the conditional probabilities along with the chain rule of probability:\n",
        "\n",
        "\\begin{align}\n",
        "p(w_1,\\ldots,w_T)&\\approx\\prod_{t=1}^T p(w_t|w_{t-1})\n",
        "\\end{align}\n",
        "\n",
        "(Here $w_0$ is `<bos>` and $w_T$ is `<eos>`)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xaZIM-5GxohT",
        "colab_type": "code",
        "outputId": "11fafda8-e3d8-478c-9716-8f20228ea540",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        }
      },
      "source": [
        "sequence = ['<bos>', 'A', 'A', 'B', '<eos>']\n",
        "\n",
        "def sequence_p(sequence, log=False):\n",
        "    total_p = 1\n",
        "\n",
        "    for i in range(len(sequence)-n+1):\n",
        "        ngram = tuple(sequence[i:i+n])\n",
        "        prefix = ngram[:-1]\n",
        "        word = ngram[-1]\n",
        "        p = count[prefix][word] / max(total[prefix], 1)\n",
        "        if log:\n",
        "            print(\"p(%s | %s) =\\t%.3f\" % (word, ', '.join(prefix), p))\n",
        "\n",
        "        total_p *= p\n",
        "    return total_p\n",
        "    \n",
        "\n",
        "print(\"\\nProduct: p(%s) = %.3f\" % (''.join(sequence[1:-1]), sequence_p(sequence, log=True)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "p(A | <bos>) =\t1.000\n",
            "p(A | A) =\t0.615\n",
            "p(B | A) =\t0.231\n",
            "p(<eos> | B) =\t0.500\n",
            "\n",
            "Product: p(AAB) = 0.071\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pesuHO_MxohV",
        "colab_type": "text"
      },
      "source": [
        "## Problem of N-gram language model:\n",
        "1- Data sparsity: lack of generalization,\n",
        " it occurs because some of n-gram do not occur in the training set but they do in the test set. Traditional solution:\n",
        " - **Smoothing**: add a small constant to avoid 0.\n",
        " - **Backoff**: try a shorter window.\n",
        " \n",
        " If $c(w_{t−n+1},\\ldots,w_t ) > 0$:\n",
        "$$P_{bo} (w_t |w_{t−n+1} , . . . , w_{t−1} ) =\n",
        "\\frac{\\text{count}(w_{t-n+1},\\ldots,w_t)}{\\text{count}(w_{t-n+1},\\ldots,w_{t-1})}$$\n",
        "else backoff to (n − 1)gram:\n",
        "$$P_{bo} (w_t |w_{t−n+1} , . . . , w_{t−1} ) =  0.4P P_{bo} (w_t |w_{t−n+2} , . . . , w_{t−1} ) $$\n",
        "Apply recursively until a existing n-gram is found\n",
        "\n",
        "    **Problem**. Probabilities do not sum to 1!\n",
        "\n",
        " - **Kneser-Ney Smoothing** is a recursive interpolation model\n",
        " \n",
        " Let  $c(w,w')$ be the number of occurrences of the word w  followed by the word w' in the corpus.\n",
        "$$p_{KN} (w_t | w_{t-1})= \\frac{max(c(w_{t-1},w_t)-\\delta, 0)}{\\sum_{w'} c(w_{t-1},w')} + \\lambda_{w_{t-1}} p_{KN}(w_t)$$\n",
        "  Where the unigram probability $p_{KN} ( w_t ) $ depends on how likely it is to see the word $w_{t}$ in an unfamiliar context, which is estimated as the number of times it appears after any other word divided by the number of distinct pairs of consecutive words in the corpus:\n",
        "    $$p_{KN} (w_t ) =   \\frac{|{w:0 <c(w,w')}|}{|{(w,w''):0< c(w,w'')}|}$$\n",
        "\n",
        "\n",
        "\n",
        "2- Inability to capture long-term dependencies \n",
        "- To capture long-term dependencies, n must be large.\n",
        "- To address data sparsity, n must be small."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QXWPKV4lxohW",
        "colab_type": "text"
      },
      "source": [
        "### Real Example: Dialogue Utterances\n",
        "\n",
        "Now lets use the same ideas on a more realistic text corpus.\n",
        "\n",
        "We will use utterances from a dialogue dataset called **Persona-Chat**. This dataset is relatively small and centers on a single domain, but it is simple and interpretable for our purposes here.\n",
        "\n",
        "We'll also use an off-the-shelf ngram modeling package called `KenLM`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xGSYZml_xohW",
        "colab_type": "text"
      },
      "source": [
        "#### Loading the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fhqOSN5qxohX",
        "colab_type": "code",
        "outputId": "e78373b0-f8ed-4c47-b55b-b062f889b4bd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 161
        }
      },
      "source": [
        "train = []\n",
        "valid = []\n",
        "import jsonlines\n",
        "\n",
        "for ds, name in [(train, 'train'), (valid, 'valid')]:\n",
        "    for line in jsonlines.Reader(open('data/personachat/personachat_all_sentences_%s.jsonl' % name, 'r')):\n",
        "        ds.append(line['tokens'])\n",
        "        \n",
        "vocab = list(set([t for ts in train for t in ts]))      \n",
        "print(\"Number of train examples: %d\" % (len(train)))\n",
        "print(\"Number of valid examples: %d\" % (len(valid)))\n",
        "print(\"Vocab size: %d\" % (len(vocab)))\n",
        "\n",
        "print(\"\\nExamples:\")\n",
        "pprint(train[:3])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of train examples: 133176\n",
            "Number of valid examples: 16181\n",
            "Vocab size: 19153\n",
            "\n",
            "Examples:\n",
            "[['i', 'am', 'doing', 'great', 'except', 'for', 'the', 'allergies', '.'],\n",
            " ['i', 'am', 'a', 'woman', 'what', 'about', 'you', '.'],\n",
            " ['i', 'thought', 'you', 'were', 'a', 'college', 'kid', '.']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kQzE-ejiyf0S",
        "colab_type": "code",
        "outputId": "a9b8be27-9014-4799-c080-0fb03d5ca872",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "!pip install jsonlines"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: jsonlines in /usr/local/lib/python3.6/dist-packages (1.2.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from jsonlines) (1.12.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CAF8J9NMxohZ",
        "colab_type": "text"
      },
      "source": [
        "### KenLM\n",
        "\n",
        "KenLM estimates n-gram language models using **modified Kneser-Ney smoothing**, and has a fast and memory-efficient implementation. \n",
        "- As we have seen above, **smoothing** is a technique used to account for ngrams that do not occur in the training corpus. \n",
        "- Normally, these ngrams would receive zero-probability mass. Smoothing ensures every ngram receives some probability.\n",
        "\n",
        "\n",
        "\n",
        "Please see page 48 of the [lecture note](https://github.com/nyu-dl/NLP_DL_Lecture_Note/blob/master/lecture_note.pdf) for an overview of Kneser-Ney smoothing, and [[Chen & Goodman 1998]](https://dash.harvard.edu/bitstream/handle/1/25104739/tr-10-98.pdf?sequence=1) for further details."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_JlM4y79xoha",
        "colab_type": "code",
        "outputId": "6b96ed6e-ecac-42bf-83d8-710551070af5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "KENLM_DIR='./kenlm'\n",
        "!ls"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "data  img  kenlm  models  ngram_lms.ipynb\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8YsZBolixohd",
        "colab_type": "text"
      },
      "source": [
        "#### Tokenize data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aCO-9_KLxohe",
        "colab_type": "code",
        "outputId": "77a9c624-b8d7-410d-dac6-43b5d1eacadf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "if not os.path.exists('data/tokenized'):\n",
        "    os.makedirs('data/tokenized')\n",
        "with open('data/tokenized/pchat_train', 'w') as f:\n",
        "    for line in tqdm(train):\n",
        "        f.write(' '.join(line))\n",
        "        f.write('\\n')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 133176/133176 [00:00<00:00, 864678.57it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lGcthN8Qxohi",
        "colab_type": "text"
      },
      "source": [
        "#### Build kenlm n-gram models\n",
        "\n",
        "This uses the `kenlm` commands `lmplz` to estimate the language model, then `build_binary` to convert it to an efficient format. We load the resulting model using the `kenlm` python wrapper.\n",
        "\n",
        "We do this for n-gram models of order `2,3,4`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dlD74TDLxohj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# cat /data/amazon_train.txt | kenlm/build/bin/lmplz -o 5 > amazonLM5.arpa\n",
        "# !pip3 install --user kenlm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LcSkcr48xohl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import kenlm\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l6bumEfdxohn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import kenlm\n",
        "\n",
        "data_prefix = 'pchat'\n",
        "dataset = 'pchat_train'\n",
        "if not os.path.exists('models'):\n",
        "    os.makedirs('models')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x2trBaNSxohq",
        "colab_type": "code",
        "outputId": "fc03ea28-8713-40f6-cdc6-7fd4b172db7c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "\n",
        "models = {}\n",
        "for n in [2,3,4]:\n",
        "    model_temp = 'models/%s_%d.arpa' % (data_prefix, n)\n",
        "    model_name = 'models/%s_%d.klm' % (data_prefix, n)\n",
        "    ! cat ./data/tokenized/$dataset | $KENLM_DIR/build/bin/lmplz -o $n > $model_temp\n",
        "    ! $KENLM_DIR/build/bin/ $model_temp $model_name\n",
        "    models[n] = kenlm.LanguageModel(model_name)\n",
        "    \n",
        "# /home/aisha/Documents/ammiGH/NLP/part2/Day1_LM/ngram_lm_lab/kenlm/build/bin"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "=== 1/5 Counting and sorting n-grams ===\n",
            "File stdin isn't normal.  Using slower read() instead of mmap().  No progress bar.\n",
            "tcmalloc: large alloc 2972459008 bytes == 0x55e660126000 @  0x7f6260d401e7 0x55e65d77b752 0x55e65d70f398 0x55e65d6ee2d0 0x55e65d6da0d6 0x7f625eed9b97 0x55e65d6dbb1a\n",
            "tcmalloc: large alloc 7926562816 bytes == 0x55e7113e8000 @  0x7f6260d401e7 0x55e65d77b752 0x55e65d7657ea 0x55e65d766208 0x55e65d6ee2ed 0x55e65d6da0d6 0x7f625eed9b97 0x55e65d6dbb1a\n",
            "Unigram tokens 1602042 types 19156\n",
            "=== 2/5 Calculating and sorting adjusted counts ===\n",
            "Chain sizes: 1:229872 2:10923950080\n",
            "tcmalloc: large alloc 10923950080 bytes == 0x55e8ea3e8000 @  0x7f6260d401e7 0x55e65d77b752 0x55e65d7657ea 0x55e65d766208 0x55e65d6ee88e 0x55e65d6da0d6 0x7f625eed9b97 0x55e65d6dbb1a\n",
            "Statistics:\n",
            "1 19156 D1=0.588857 D2=1.0348 D3+=1.3388\n",
            "2 231267 D1=0.708637 D2=1.06132 D3+=1.37629\n",
            "Memory estimate for binary LM:\n",
            "type      kB\n",
            "probing 4551 assuming -p 1.5\n",
            "probing 4626 assuming -r models -p 1.5\n",
            "trie    1747 without quantization\n",
            "trie    1099 assuming -q 8 -b 8 quantization \n",
            "trie    1747 assuming -a 22 array pointer compression\n",
            "trie    1099 assuming -a 22 -q 8 -b 8 array pointer compression and quantization\n",
            "=== 3/5 Calculating and sorting initial probabilities ===\n",
            "Chain sizes: 1:229872 2:3700272\n",
            "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
            "####################################################################################################\n",
            "=== 4/5 Calculating and writing order-interpolated probabilities ===\n",
            "Chain sizes: 1:229872 2:3700272\n",
            "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
            "####################################################################################################\n",
            "=== 5/5 Writing ARPA model ===\n",
            "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
            "****************************************************************************************************\n",
            "Name:lmplz\tVmPeak:21525056 kB\tVmRSS:2939572 kB\tRSSMax:2949400 kB\tuser:0.766356\tsys:1.79941\tCPU:2.56581\treal:2.57022\n",
            "/bin/bash: ./kenlm/build/bin/: Is a directory\n",
            "=== 1/5 Counting and sorting n-grams ===\n",
            "File stdin isn't normal.  Using slower read() instead of mmap().  No progress bar.\n",
            "tcmalloc: large alloc 2515165184 bytes == 0x5625b263c000 @  0x7f952f1e51e7 0x5625b0863752 0x5625b07f7398 0x5625b07d62d0 0x5625b07c20d6 0x7f952d37eb97 0x5625b07c3b1a\n",
            "tcmalloc: large alloc 8383864832 bytes == 0x5626484e2000 @  0x7f952f1e51e7 0x5625b0863752 0x5625b084d7ea 0x5625b084e208 0x5625b07d62ed 0x5625b07c20d6 0x7f952d37eb97 0x5625b07c3b1a\n",
            "Unigram tokens 1602042 types 19156\n",
            "=== 2/5 Calculating and sorting adjusted counts ===\n",
            "Chain sizes: 1:229872 2:3799634944 3:7124315648\n",
            "tcmalloc: large alloc 7124320256 bytes == 0x5625b253a000 @  0x7f952f1e51e7 0x5625b0863752 0x5625b084d7ea 0x5625b084e208 0x5625b07d688e 0x5625b07c20d6 0x7f952d37eb97 0x5625b07c3b1a\n",
            "tcmalloc: large alloc 3799638016 bytes == 0x56283c8fc000 @  0x7f952f1e51e7 0x5625b0863752 0x5625b084d7ea 0x5625b084e208 0x5625b07d6c7d 0x5625b07c20d6 0x7f952d37eb97 0x5625b07c3b1a\n",
            "Statistics:\n",
            "1 19156 D1=0.588857 D2=1.0348 D3+=1.3388\n",
            "2 231267 D1=0.722265 D2=1.09097 D3+=1.44737\n",
            "3 617624 D1=0.798924 D2=1.0703 D3+=1.33013\n",
            "Memory estimate for binary LM:\n",
            "type       kB\n",
            "probing 16763 assuming -p 1.5\n",
            "probing 18193 assuming -r models -p 1.5\n",
            "trie     6683 without quantization\n",
            "trie     3625 assuming -q 8 -b 8 quantization \n",
            "trie     6354 assuming -a 22 array pointer compression\n",
            "trie     3296 assuming -a 22 -q 8 -b 8 array pointer compression and quantization\n",
            "=== 3/5 Calculating and sorting initial probabilities ===\n",
            "Chain sizes: 1:229872 2:3700272 3:12352480\n",
            "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
            "####################################################################################################\n",
            "=== 4/5 Calculating and writing order-interpolated probabilities ===\n",
            "Chain sizes: 1:229872 2:3700272 3:12352480\n",
            "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
            "####################################################################################################\n",
            "=== 5/5 Writing ARPA model ===\n",
            "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
            "****************************************************************************************************\n",
            "Name:lmplz\tVmPeak:14641496 kB\tVmRSS:2511352 kB\tRSSMax:2511472 kB\tuser:1.42569\tsys:1.64395\tCPU:3.06968\treal:3.0303\n",
            "/bin/bash: ./kenlm/build/bin/: Is a directory\n",
            "=== 1/5 Counting and sorting n-grams ===\n",
            "File stdin isn't normal.  Using slower read() instead of mmap().  No progress bar.\n",
            "tcmalloc: large alloc 2179809280 bytes == 0x55612065e000 @  0x7f0e7ec351e7 0x55611ef09752 0x55611ee9d398 0x55611ee7c2d0 0x55611ee680d6 0x7f0e7cdceb97 0x55611ee69b1a\n",
            "tcmalloc: large alloc 8719212544 bytes == 0x5561a2532000 @  0x7f0e7ec351e7 0x55611ef09752 0x55611eef37ea 0x55611eef4208 0x55611ee7c2ed 0x55611ee680d6 0x7f0e7cdceb97 0x55611ee69b1a\n",
            "Unigram tokens 1602042 types 19156\n",
            "=== 2/5 Calculating and sorting adjusted counts ===\n",
            "Chain sizes: 1:229872 2:1859395712 3:3486366976 4:5578187264\n",
            "tcmalloc: large alloc 5578194944 bytes == 0x55612055c000 @  0x7f0e7ec351e7 0x55611ef09752 0x55611eef37ea 0x55611eef4208 0x55611ee7c88e 0x55611ee680d6 0x7f0e7cdceb97 0x55611ee69b1a\n",
            "tcmalloc: large alloc 1859403776 bytes == 0x55626cd66000 @  0x7f0e7ec351e7 0x55611ef09752 0x55611eef37ea 0x55611eef4208 0x55611ee7cc7d 0x55611ee680d6 0x7f0e7cdceb97 0x55611ee69b1a\n",
            "tcmalloc: large alloc 3486367744 bytes == 0x5563aa920000 @  0x7f0e7ec351e7 0x55611ef09752 0x55611eef37ea 0x55611eef4208 0x55611ee7cc7d 0x55611ee680d6 0x7f0e7cdceb97 0x55611ee69b1a\n",
            "Statistics:\n",
            "1 19156 D1=0.588857 D2=1.0348 D3+=1.3388\n",
            "2 231267 D1=0.722265 D2=1.09097 D3+=1.44737\n",
            "3 617624 D1=0.821472 D2=1.14168 D3+=1.40217\n",
            "4 932153 D1=0.863174 D2=1.12888 D3+=1.29333\n",
            "Memory estimate for binary LM:\n",
            "type       kB\n",
            "probing 36767 assuming -p 1.5\n",
            "probing 41816 assuming -r models -p 1.5\n",
            "trie    15838 without quantization\n",
            "trie     8356 assuming -q 8 -b 8 quantization \n",
            "trie    14567 assuming -a 22 array pointer compression\n",
            "trie     7085 assuming -a 22 -q 8 -b 8 array pointer compression and quantization\n",
            "=== 3/5 Calculating and sorting initial probabilities ===\n",
            "Chain sizes: 1:229872 2:3700272 3:12352480 4:22371672\n",
            "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
            "####################################################################################################\n",
            "=== 4/5 Calculating and writing order-interpolated probabilities ===\n",
            "Chain sizes: 1:229872 2:3700272 3:12352480 4:22371672\n",
            "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
            "####################################################################################################\n",
            "=== 5/5 Writing ARPA model ===\n",
            "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
            "****************************************************************************************************\n",
            "Name:lmplz\tVmPeak:14417516 kB\tVmRSS:2224212 kB\tRSSMax:2224292 kB\tuser:2.33743\tsys:1.69375\tCPU:4.03122\treal:3.65207\n",
            "/bin/bash: ./kenlm/build/bin/: Is a directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ldWu1QdCxohu",
        "colab_type": "text"
      },
      "source": [
        "## Evaluation\n",
        "\n",
        "### Perplexity\n",
        "\n",
        "Intuitively, a good model should assign high probabilities to sequences from the 'true' distribution that it is modeling.\n",
        "\n",
        "A common way of quantifying this is with **perplexity**, a metric inversely-proportional to the probability that the model assigns to a set of sequences, e.g. a 'test set':\n",
        "\n",
        "\\begin{align}\n",
        "\\large \\text{ppl}(p, D) &\\large\\ = 2^{-\\frac{1}{N_{total}}\\log_2 p(D)}\n",
        "\\end{align}\n",
        "\n",
        "\n",
        "where $D=\\{(w_1,\\ldots,w_{N_i})_i\\}_{i=1}^M$ is a dataset of $M$ sequences with total length $N_{\\text{total}}=\\sum_{i}N_i$.\n",
        "\n",
        "Perplexity is defined on $[1,\\infty)$, with 1 being a perfect model (assigning probability 1 to $D$), and a 'worse' model as perplexity increases.\n",
        "\n",
        "Intuitively, _perplexity measures the average rank of the true next-token, when tokens are ordered by the model's conditional probabilities_.\n",
        "\n",
        "\n",
        "<!-- Section 1.3 of [[Chen & Goodman 1998]](https://dash.harvard.edu/bitstream/handle/1/25104739/tr-10-98.pdf?sequence=1) has a concise summary of perplexity and its motivation. !-->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "08ow1NBHxohu",
        "colab_type": "text"
      },
      "source": [
        "#### Evaluate Perplexity\n",
        "\n",
        "`kenlm` outputs log probabilities in **log base 10**. For the standard definition of perplexity we need **log base 2**. See the code below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pFnuHhi_xohu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def perplexity_kenlm(model, sequences):\n",
        "    n_total = 0\n",
        "    logp_total = 0\n",
        "    for sequence in sequences:\n",
        "        text = ' '.join(sequence)\n",
        "        logp_total += model.score(text)\n",
        "        n_total += len(sequence) + 1  # add 1 for </s>\n",
        "        \n",
        "    # Convert log10 to log2\n",
        "    log2p_total = logp_total / np.log10(2)\n",
        "    ppl = 2 ** (- (1.0 / n_total) * log2p_total)\n",
        "    return ppl"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i4get-mZxohx",
        "colab_type": "code",
        "outputId": "0c0057b8-0b48-43b3-9d1c-c9cd02126ba7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "print(\"=== Train ===\")\n",
        "df = pd.DataFrame([(n, perplexity_kenlm(models[n], train)) for n in [2, 3, 4]], columns=['n', 'ppl'])\n",
        "df.style.hide_index()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "=== Train ===\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<style  type=\"text/css\" >\n",
              "</style><table id=\"T_ea7759c8_6ceb_11ea_a31b_0242ac1c0002\" ><thead>    <tr>        <th class=\"col_heading level0 col0\" >n</th>        <th class=\"col_heading level0 col1\" >ppl</th>    </tr></thead><tbody>\n",
              "                <tr>\n",
              "                                <td id=\"T_ea7759c8_6ceb_11ea_a31b_0242ac1c0002row0_col0\" class=\"data row0 col0\" >2</td>\n",
              "                        <td id=\"T_ea7759c8_6ceb_11ea_a31b_0242ac1c0002row0_col1\" class=\"data row0 col1\" >39.9248</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                                <td id=\"T_ea7759c8_6ceb_11ea_a31b_0242ac1c0002row1_col0\" class=\"data row1 col0\" >3</td>\n",
              "                        <td id=\"T_ea7759c8_6ceb_11ea_a31b_0242ac1c0002row1_col1\" class=\"data row1 col1\" >15.462</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                                <td id=\"T_ea7759c8_6ceb_11ea_a31b_0242ac1c0002row2_col0\" class=\"data row2 col0\" >4</td>\n",
              "                        <td id=\"T_ea7759c8_6ceb_11ea_a31b_0242ac1c0002row2_col1\" class=\"data row2 col1\" >8.91961</td>\n",
              "            </tr>\n",
              "    </tbody></table>"
            ],
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x7f76939161d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MoI0quXdxohy",
        "colab_type": "code",
        "outputId": "79a7d269-5d84-470d-9d27-349ef8a05560",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "print(\"=== Valid ===\")\n",
        "df = pd.DataFrame([(n, perplexity_kenlm(models[n], valid)) for n in [2, 3, 4]], columns=['n', 'ppl'])\n",
        "df.style.hide_index()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "=== Valid ===\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<style  type=\"text/css\" >\n",
              "</style><table id=\"T_ea98458e_6ceb_11ea_a31b_0242ac1c0002\" ><thead>    <tr>        <th class=\"col_heading level0 col0\" >n</th>        <th class=\"col_heading level0 col1\" >ppl</th>    </tr></thead><tbody>\n",
              "                <tr>\n",
              "                                <td id=\"T_ea98458e_6ceb_11ea_a31b_0242ac1c0002row0_col0\" class=\"data row0 col0\" >2</td>\n",
              "                        <td id=\"T_ea98458e_6ceb_11ea_a31b_0242ac1c0002row0_col1\" class=\"data row0 col1\" >59.188</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                                <td id=\"T_ea98458e_6ceb_11ea_a31b_0242ac1c0002row1_col0\" class=\"data row1 col0\" >3</td>\n",
              "                        <td id=\"T_ea98458e_6ceb_11ea_a31b_0242ac1c0002row1_col1\" class=\"data row1 col1\" >44.9771</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                                <td id=\"T_ea98458e_6ceb_11ea_a31b_0242ac1c0002row2_col0\" class=\"data row2 col0\" >4</td>\n",
              "                        <td id=\"T_ea98458e_6ceb_11ea_a31b_0242ac1c0002row2_col1\" class=\"data row2 col1\" >43.2188</td>\n",
              "            </tr>\n",
              "    </tbody></table>"
            ],
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x7f76b24cbef0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "paiHyIMfxoh1",
        "colab_type": "text"
      },
      "source": [
        "#### Sequence probabilities:\n",
        "\\begin{align}\n",
        "p(w_1,\\ldots,w_T)&\\approx\\prod_{t=1}^T p(w_t|w_{t-2},w_{t-1})\\\\\n",
        "&=\\sum_{t=1}^T \\log p(w_t|w_{t-2},w_{t-1}).\n",
        "\\end{align}\n",
        "\n",
        "where we use log probabilities in practice to avoid a product of many small numbers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JLmX2faGxoh2",
        "colab_type": "code",
        "outputId": "dbdd0c0a-78d7-4bca-a170-03a28d41777d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 467
        }
      },
      "source": [
        "sentences = [\n",
        "    'i like my pet dog .',\n",
        "    'i like my pet zebra .',\n",
        "    'i like my pet lion .',\n",
        "    'i live in the united states .',\n",
        "    'i live in the united states of america .'\n",
        "]\n",
        "\n",
        "for sentence in sentences:\n",
        "    print(sentence)\n",
        "    for n in [2, 3, 4]:\n",
        "        log10p = models[n].score(sentence)\n",
        "        log2p = log10p / np.log10(2)\n",
        "        print(\"n: %d\\t logp: %.3f\" % (n, log2p))\n",
        "    print()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "i like my pet dog .\n",
            "n: 2\t logp: -29.216\n",
            "n: 3\t logp: -28.843\n",
            "n: 4\t logp: -29.219\n",
            "\n",
            "i like my pet zebra .\n",
            "n: 2\t logp: -32.303\n",
            "n: 3\t logp: -31.157\n",
            "n: 4\t logp: -32.101\n",
            "\n",
            "i like my pet lion .\n",
            "n: 2\t logp: -38.352\n",
            "n: 3\t logp: -41.017\n",
            "n: 4\t logp: -42.320\n",
            "\n",
            "i live in the united states .\n",
            "n: 2\t logp: -22.599\n",
            "n: 3\t logp: -20.095\n",
            "n: 4\t logp: -17.854\n",
            "\n",
            "i live in the united states of america .\n",
            "n: 2\t logp: -40.958\n",
            "n: 3\t logp: -26.627\n",
            "n: 4\t logp: -24.311\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NgRveW0wxoh4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jbJXokBBxoh6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}